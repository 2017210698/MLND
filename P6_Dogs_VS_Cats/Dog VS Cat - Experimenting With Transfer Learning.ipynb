{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 猫狗大战 - 迁移学习的一些尝试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 模型的训练方式\n",
    "\n",
    "- 深度学习模型可以划分为 **训练** 和 **预测** 两个阶段。\n",
    "\n",
    "\n",
    "- **训练** 又分为两种策略。一种是白手起家从头搭建模型进行训练，一种是通过预训练模型进行训练。\n",
    "\n",
    "\n",
    "- **预测** 相对简单，直接用已经训练好的模型对数据集进行预测即可。（此时采用上面两种策略训练得到的模型都被视为“已训练模型”）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Application}\n",
    "\\begin{cases}\n",
    "    \\textbf{Training}\n",
    "    \\begin{cases}\n",
    "        \\color{Blue}{\\textbf{Training From Scratch}} \\\\[2ex]\n",
    "        \\color{Green}{\\textbf{Using Pre-trained Model}}\n",
    "        \\begin{cases}\n",
    "            ^{\\#} \\textrm{1. Transfer Learning} \\\\[2ex]\n",
    "            ^{\\#} \\textrm{2. Extract Feature Vector (Bottle Neck)} \\\\[2ex]\n",
    "            ^{\\#} \\textrm{3. Fine-tune}\n",
    "        \\end{cases}\n",
    "    \\end{cases}\n",
    "    \\\\[3ex]\n",
    "    \\textbf{Inference}: \\textrm{Using Pre-trained Model}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 迁移学习的三种训练方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$^{\\#} \\textbf{1. Transfer Learning}$：冻结预训练模型的全部卷积层，只训练自己定制的全连接层。\n",
    "\n",
    "\n",
    "$^{\\#} \\textbf{2. Extract Feature Vector}$：先计算出预训练模型的卷积层对所有训练和测试数据的特征向量，然后抛开预训练模型，只训练自己定制的全连接层。\n",
    "\n",
    "\n",
    "$^{\\#} \\textbf{3. Fine-tune}$：冻结预训练模型的部分卷积层（通常是靠近输入的大部分卷积层），训练剩下的卷积层（通常是靠近输出的小部分卷积层）和全连接层。\n",
    "\n",
    "> “迁移学习” 这个名词既可以广义的指代以上全部三种方式，也可以狭义的指具体第一种方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 三种训练方式的对比\n",
    "\n",
    "- 第一种和第二种训练得到的模型本质上并没有什么区别，但是第二种的计算复杂度要远远好于第一种。\n",
    "\n",
    "\n",
    "- 第三种是对前两种方法的补充，以进一步提升模型性能。要注意的是，这种方法**并不一定能对模型有所提升**。\n",
    "\n",
    "\n",
    "- 本质上来讲，这三种迁移学习的方式都是为了让预训练模型能够胜任新数据集的识别工作，能够让预训练模型原本的特征提取能力得到充分的释放和利用。但是，在此基础上如果想让模型能够达到更低的Loss，那么光靠迁移学习是不够的，靠的更多的还是**模型的结构以及新数据集的数量**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from utils import get_params_count\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.applications import inception_v3, xception, resnet50, vgg16, vgg19\n",
    "from keras.applications import InceptionV3, Xception, ResNet50, VGG16, VGG19\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 12500/12500 [00:26<00:00, 470.66it/s]\n",
      "100%|█████████████████████████████████████████████████████| 12500/12500 [00:27<00:00, 459.49it/s]\n",
      "100%|█████████████████████████████████████████████████████| 12500/12500 [00:26<00:00, 465.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size = 6.24 GB\n",
      "Testing Data Size = 3.12 GB\n"
     ]
    }
   ],
   "source": [
    "height = 299\n",
    "labels = np.array([0] * 12500 + [1] * 12500)\n",
    "train = np.zeros((25000, height, height, 3), dtype=np.uint8)\n",
    "test = np.zeros((12500, height, height, 3), dtype=np.uint8)\n",
    "\n",
    "for i in tqdm(range(12500)):\n",
    "    img = cv2.imread('./train/cat/%s.jpg' % str(i))\n",
    "    img = cv2.resize(img, (height, height))\n",
    "    train[i] = img[:, :, ::-1]\n",
    "    \n",
    "for i in tqdm(range(12500)):\n",
    "    img = cv2.imread('./train/dog/%s.jpg' % str(i))\n",
    "    img = cv2.resize(img, (height, height))\n",
    "    train[i + 12500] = img[:, :, ::-1]\n",
    "\n",
    "for i in tqdm(range(12500)):\n",
    "    img = cv2.imread('./test/%s.jpg' % str(i + 1))\n",
    "    img = cv2.resize(img, (height, height))\n",
    "    test[i] = img[:, :, ::-1]\n",
    "    \n",
    "print('Training Data Size = %.2f GB' % (sys.getsizeof(train)/1024**3))\n",
    "print('Testing Data Size = %.2f GB' % (sys.getsizeof(test)/1024**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验一：冻结全部卷积层 + 训练自己定制的全连接层\n",
    "\n",
    "#### 要点\n",
    "\n",
    "- **使用InceptionV3预训练模型**：该模型当初训练时的输入图像尺寸是299x299，且图像通道顺序为RGB，模型总参数为**两千多万个**。\n",
    "- **预处理**：按照预训练模型原本的预处理方式对数据进行预处理。\n",
    "- **基模型**：导入预训练模型，注意只导入卷积层部分，并锁定全部卷积层参数。\n",
    "- **定制模型**：卷积层之后先接全局平均池化（GAP），再接Dropout，再接分类器，根据分类任务选择输出个数。模型可训练参数只有**两千个**。\n",
    "- **优化器**：采用较小学习率的SGD。\n",
    "- **数据准备**：将训练集划分为训练集和验证集。\n",
    "- **定义回调函数以方便训练**：自动在每代结束保存模型，以val_loss作为监控指标进行早停，训练历史数据同步更新到Tensorboard供可视化。\n",
    "- **需要用非常小的batch_size进行训练**：这样可以让模型更快更好的收敛。\n",
    "- 可以看到，虽然卷积层全都已经锁定，但是由于样本依然需要从模型的输入一直计算到输出，因此训练还是比较耗时的，训练五代需要十几分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 2049, Non-Trainable: 21802784\n"
     ]
    }
   ],
   "source": [
    "# Preprocess: Standardization\n",
    "x = Input(shape=(height, height, 3))\n",
    "x = Lambda(inception_v3.preprocess_input)(x)\n",
    "\n",
    "# Base Model: Freeze all conv layers\n",
    "base_model = InceptionV3(include_top=False, input_tensor=x, weights='imagenet', pooling='avg')\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Customized Classifier\n",
    "y = Dropout(0.2)(base_model.output)\n",
    "y = Dense(1, activation='sigmoid', kernel_initializer='he_normal')(y)\n",
    "\n",
    "# Full Model: Pre-train Conv + Customized Classifier\n",
    "model = Model(inputs=base_model.input, outputs=y, name='Transfer_Learning')\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print('Trainable: %d, Non-Trainable: %d' % get_params_count(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train, labels, shuffle=True, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 162s - loss: 0.1837 - acc: 0.9415 - val_loss: 0.0875 - val_acc: 0.9786\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 160s - loss: 0.1214 - acc: 0.9564 - val_loss: 0.0666 - val_acc: 0.9832\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 160s - loss: 0.1088 - acc: 0.9599 - val_loss: 0.0617 - val_acc: 0.9828\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 159s - loss: 0.1083 - acc: 0.9595 - val_loss: 0.0684 - val_acc: 0.9760\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 159s - loss: 0.0995 - acc: 0.9646 - val_loss: 0.0548 - val_acc: 0.9842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1899d354e10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare Callbacks for Model Checkpoint, Early Stopping and Tensorboard.\n",
    "log_name = '/DogVSCat-EP{epoch:02d}-LOSS{val_loss:.4f}.h5'\n",
    "log_dir = datetime.now().strftime('transfer_model_%Y%m%d_%H%M')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=20)\n",
    "mc = ModelCheckpoint(log_dir + log_name, monitor='val_loss', save_best_only=True)\n",
    "tb = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=16, epochs=5, validation_data=(X_val, y_val), callbacks=[es, mc, tb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 实验二：导出特征向量，再单独训练分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 要点\n",
    "\n",
    "- **预处理**：导出训练集和测试集的特征向量之前，一定要记得按照预训练模型的要求进行预处理，否则导出的特征将不是模型的最佳表现。\n",
    "- **基模型**：基模型同样由InceptionV3的卷积层部分以及全局平均池化（GAP）构成。\n",
    "- **导出即预测**：所谓导出，其实就是让基模型直接对训练集和测试集进行预测，只不过预测出的不是类别，而是特征向量（特征图的浓缩版本）。\n",
    "- **导出需要一定时间**：由于导出需要对数据集的所有图片进行预测，因此通常需要一两分钟才能完成。好的是，一旦完成，就可以完全和CNN拜拜了。\n",
    "- **新模型的输入是特征向量**：新模型的输入不再是训练集的图像本身，而是经过预训练模型“消化”后的图像特征向量，该向量的第一个维度对应于每一个图像样本，其长度为样本的个数，第二个维度是基模型最后一层每个卷积核输出特征图的平均值，对于InceptionV3来说，第二个维度的长度是2048。\n",
    "- **划分训练集和验证集**：为了训练时了解模型收敛情况，同样对特征向量划分训练集和验证集。\n",
    "- **定制新模型**：由于已经导出特征向量，因此接下来只需训练一个输入特征长度为2048的全连接网络即可。\n",
    "- 同样采用回调函数以及很小的batch_size（16）进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess: Standardization\n",
    "x = Input(shape=(height, height, 3))\n",
    "x = Lambda(inception_v3.preprocess_input)(x)\n",
    "\n",
    "# Base Model: Extract feature vector of both train & test dataset\n",
    "base_model = InceptionV3(include_top=False, input_tensor=x, weights='imagenet', pooling='avg')\n",
    "train_gap = base_model.predict(train, batch_size=128)\n",
    "test_gap = base_model.predict(test, batch_size=128)\n",
    "\n",
    "X_train_gap, X_val_gap, y_train_gap, y_val_gap = train_test_split(train_gap, labels, shuffle=True, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 2049, Non-Trainable: 0\n"
     ]
    }
   ],
   "source": [
    "# Input Shape: (Batch Size, Feature Vector length)\n",
    "x = Input(shape=(X_train_gap.shape[1],))\n",
    "y = Dropout(0.2)(x)\n",
    "y = Dense(1, activation='sigmoid', kernel_initializer='he_normal', name='classifier')(y)\n",
    "model_gap = Model(inputs=x, outputs=y, name='GAP')\n",
    "model_gap.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "print('Trainable: %d, Non-Trainable: %d' % get_params_count(model_gap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 3s - loss: 0.0807 - acc: 0.9815 - val_loss: 0.0316 - val_acc: 0.9914\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 3s - loss: 0.0264 - acc: 0.9927 - val_loss: 0.0256 - val_acc: 0.9920\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 3s - loss: 0.0230 - acc: 0.9929 - val_loss: 0.0238 - val_acc: 0.9924\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 3s - loss: 0.0204 - acc: 0.9938 - val_loss: 0.0230 - val_acc: 0.9924\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 3s - loss: 0.0197 - acc: 0.9942 - val_loss: 0.0228 - val_acc: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18548f889b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare Callbacks for Model Checkpoint, Early Stopping and Tensorboard.\n",
    "log_name = '/DogVSCat-EP{epoch:02d}-LOSS{val_loss:.4f}.h5'\n",
    "log_dir = datetime.now().strftime('gap_model_%Y%m%d_%H%M')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=20)\n",
    "mc = ModelCheckpoint(log_dir + log_name, monitor='val_loss', save_best_only=True)\n",
    "tb = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model_gap.fit(x=X_train_gap, y=y_train_gap, batch_size=16, epochs=5, validation_data=(X_val_gap, y_val_gap), callbacks=[es, mc, tb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
